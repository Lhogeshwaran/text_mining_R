#Part 3 tfidf
#same corpus as in previous exercise
##assumes tm, ggplot and wordcloud libraries are loaded

dtm_tfidf <- DocumentTermMatrix(docs,control = list(weighting = weightTfIdf))
#note that the weighting is normalised by default (that is, the term frequencies in a
#document are normalised by the number of terms in the document)
#summary
dtm_tfidf
#inspect segment of document term matrix
inspect(dtm_tfidf[1:10,1000:1006])
#collapse matrix by summing over columns - this gets total weights (over all docs) for each term
wt_tot_tfidf <- colSums(as.matrix(dtm_tfidf))
#length should be total number of terms
length(wt_tot_tfidf )
#create sort order (asc)
ord_tfidf <- order(wt_tot_tfidf,decreasing=TRUE)
#inspect most frequently occurring terms
wt_tot_tfidf[head(ord_tfidf)]
#write to disk and inspect file
write.csv(file="wt_tot_tfidf.csv",wt_tot_tfidf[ord_tfidf])
#inspect least weighted terms
wt_tot_tfidf[tail(ord_tfidf)]

#correlations - compare to dtm generated by  tf and tf/truncated weighting
#"project" at correlation level of 0.6
findAssocs(dtm_tfidf,"govern",0.5)
findAssocs(dtm_tfidf,"system",0.8)
#notice the difference!

#histogram
wf=data.frame(term=names(wt_tot_tfidf),weights=wt_tot_tfidf)
#library(ggplot2)
p <- ggplot(subset(wf, wt_tot_tfidf>.1), aes(reorder(term,weights), weights))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
#wordcloud
#library(wordcloud)
#setting the same seed each time ensures consistent look across clouds
set.seed(42)
#limit words by specifying min total wt
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf, max.words=100)
#...add color
wordcloud(names(wt_tot_tfidf),wt_tot_tfidf,max.words=100,colors=brewer.pal(6,"Dark2"))
#play with different values of max.words
#try specifying min.freq instead of max.words